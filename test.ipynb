{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "import yaml\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    LoraConfig,\n",
    "    IA3Config,\n",
    "    AdaLoraConfig,\n",
    "    TaskType,\n",
    "    PeftType\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#  ConfigParserç±»ç”¨äºè§£æYAMLé…ç½®æ–‡ä»¶\n",
    "# ==============================================================================\n",
    "class ConfigParser:\n",
    "    def __init__(self, config_file):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–é…ç½®è§£æå™¨\n",
    "        \n",
    "        Args:\n",
    "            config_file (str): YAMLé…ç½®æ–‡ä»¶çš„è·¯å¾„\n",
    "        \"\"\"\n",
    "        self.config_file = config_file\n",
    "        self.config = self._load_config()\n",
    "        \n",
    "    def _load_config(self):\n",
    "        \"\"\"åŠ è½½YAMLé…ç½®æ–‡ä»¶\"\"\"\n",
    "        if not os.path.exists(self.config_file):\n",
    "            raise FileNotFoundError(f\"é…ç½®æ–‡ä»¶ {self.config_file} ä¸å­˜åœ¨\")\n",
    "            \n",
    "        with open(self.config_file, 'r') as f:\n",
    "            config = yaml.safe_load(f)# yaml.safe_load()è¿”å›ä¸€ä¸ªå­—å…¸ï¼Œè¯¥å­—å…¸åŒ…å«é…ç½®æ–‡ä»¶ä¸­çš„æ‰€æœ‰é”®å€¼å¯¹\n",
    "        return config\n",
    "    \n",
    "    def get_experiment_config(self):\n",
    "        \"\"\"è·å–å®éªŒåŸºæœ¬é…ç½®\"\"\"\n",
    "        exp_config = self.config.get('experiment')\n",
    "        return {\n",
    "            'model_name': exp_config.get('model_name'),\n",
    "            'dataset_name': exp_config.get('dataset_name'),\n",
    "            'task_type': exp_config.get('task_type')\n",
    "        }\n",
    "    \n",
    "    def get_peft_config(self):\n",
    "        \"\"\"è·å–PEFTæ–¹æ³•å’Œå‚æ•°é…ç½®\"\"\"\n",
    "        peft_config = self.config.get('peft')\n",
    "        return {\n",
    "            'method': peft_config.get('method'),\n",
    "            'params': peft_config.get('params')\n",
    "        }\n",
    "    \n",
    "    def get_training_config(self):\n",
    "        \"\"\"è·å–è®­ç»ƒå‚æ•°é…ç½®\"\"\"\n",
    "        return self.config.get('training')\n",
    "    \n",
    "    def get_tuner_config(self):\n",
    "        \"\"\"è·å–è¶…å‚æ•°ä¼˜åŒ–å™¨é…ç½®\"\"\"\n",
    "        tuner_config = self.config.get('tuner')\n",
    "        return {\n",
    "            'direction': tuner_config.get('direction'),\n",
    "            'n_trials': tuner_config.get('n_trials'),\n",
    "            'study_name': tuner_config.get('study_name')\n",
    "        }\n",
    "    \n",
    "    def get_peft_search_space(self):\n",
    "        \"\"\"è·å–PEFTæ–¹æ³•çš„æœç´¢ç©ºé—´é…ç½®\"\"\"\n",
    "        return self.config.get('search_space').get('peft')\n",
    "    \n",
    "    def get_training_search_space(self):\n",
    "        \"\"\"è·å–è®­ç»ƒå‚æ•°çš„æœç´¢ç©ºé—´é…ç½®\"\"\"\n",
    "        return self.config.get('search_space').get('training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#  PEFTExperimentç±»ç”¨äºæ‰§è¡ŒPEFTè®­ç»ƒå’Œè¯„ä¼°\n",
    "# ==============================================================================\n",
    "class PEFTExperiment:\n",
    "    def __init__(self, model_name, dataset_name, task_type):\n",
    "        self.model_name = model_name\n",
    "        self.dataset_name = dataset_name\n",
    "        self.task_type = task_type\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            print(\"pad_token is None, set to eos_token\")\n",
    "    \n",
    "    def get_peft_config(self, method, **kwargs):\n",
    "        \"\"\"è·å–ä¸åŒPEFTæ–¹æ³•çš„é…ç½®\"\"\"\n",
    "        configs = {\n",
    "            \"lora\": LoraConfig(\n",
    "                task_type=self.task_type,\n",
    "                inference_mode=False,\n",
    "                r=kwargs.get(\"r\"),\n",
    "                lora_alpha=kwargs.get(\"lora_alpha\"),# å†³å®šdelta_Wçš„å½±å“\n",
    "                lora_dropout=kwargs.get(\"lora_dropout\"),\n",
    "                target_modules=kwargs.get(\"target_modules\")\n",
    "            ),\n",
    "            \"ia3\": IA3Config(\n",
    "                task_type=self.task_type,\n",
    "                inference_mode=False,\n",
    "                target_modules=kwargs.get(\"target_modules\"),\n",
    "                feedforward_modules=kwargs.get(\"feedforward_modules\")\n",
    "            ),\n",
    "            \"adalora\": AdaLoraConfig(\n",
    "                task_type=self.task_type,\n",
    "                inference_mode=False,\n",
    "                r=kwargs.get(\"r\"),\n",
    "                lora_alpha=kwargs.get(\"lora_alpha\"),\n",
    "                target_r=kwargs.get(\"target_r\"),\n",
    "                init_r=kwargs.get(\"init_r\"),\n",
    "                tinit=kwargs.get(\"tinit\"),\n",
    "                tfinal=kwargs.get(\"tfinal\"),\n",
    "                deltaT=kwargs.get(\"deltaT\"),\n",
    "                lora_dropout=kwargs.get(\"lora_dropout\"),\n",
    "                target_modules=kwargs.get(\"target_modules\")\n",
    "            )\n",
    "        }\n",
    "        return configs[method.lower()]\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        \"\"\"å‡†å¤‡æ•°æ®é›†\"\"\"\n",
    "        dataset = load_dataset(self.dataset_name)\n",
    "        \n",
    "        def tokenize_function(examples):\n",
    "            return self.tokenizer(\n",
    "                examples[\"text\"] if \"text\" in examples else examples[\"sentence\"],\n",
    "                truncation=True,\n",
    "                padding=False,\n",
    "                max_length=512\n",
    "            )\n",
    "        \n",
    "        tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "        return tokenized_dataset\n",
    "    \n",
    "    def train_with_peft(self, peft_method, peft_params, training_params):\n",
    "        \"\"\"ä½¿ç”¨PEFTæ–¹æ³•è®­ç»ƒæ¨¡å‹\"\"\"\n",
    "\n",
    "        # åˆå§‹åŒ–wandb\n",
    "        run = wandb.init(\n",
    "            project=\"DNA_LLM_finetune\",\n",
    "            name=f\"{peft_method}_{self.model_name}_{self.dataset_name}\",\n",
    "            config={\n",
    "                \"model\": self.model_name,\n",
    "                \"peft_method\": peft_method,\n",
    "                \"peft_params\": peft_params,#å‚æ•°é«˜æ•ˆå¾®è°ƒè¶…å‚æ•°\n",
    "                **training_params#è®­ç»ƒè¶…å‚æ•°\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # åŠ è½½æ¨¡å‹\n",
    "        model = AutoModel.from_pretrained(\n",
    "            self.model_name,\n",
    "            num_labels=2  # æ ¹æ®ä»»åŠ¡è°ƒæ•´\n",
    "        )\n",
    "        \n",
    "        # åº”ç”¨PEFTé…ç½®\n",
    "        peft_config = self.get_peft_config(peft_method, **peft_params)\n",
    "        model = get_peft_model(model, peft_config)#åº”ç”¨é…ç½®åçš„PEFTæ¨¡å‹\n",
    "        \n",
    "        # å‡†å¤‡æ•°æ®\n",
    "        dataset = self.prepare_data()\n",
    "        data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)#åŠ¨æ€paddingä¸€ä¸ªbatchä¸­çš„æ‰€æœ‰æ ·æœ¬\n",
    "        \n",
    "        # è®­ç»ƒå‚æ•°\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"./results/{peft_method}_{run.id}\",\n",
    "            learning_rate=training_params.get(\"learning_rate\"),\n",
    "            per_device_train_batch_size=training_params.get(\"batch_size\"),\n",
    "            num_train_epochs=training_params.get(\"epochs\"),\n",
    "            weight_decay=training_params.get(\"weight_decay\"),\n",
    "            logging_steps=10,\n",
    "            evaluation_strategy=\"epoch\",# å®šä¹‰evaluationæ—¶æœº\n",
    "            save_strategy=\"epoch\",# å®šä¹‰æ¨¡å‹ä¿å­˜æ—¶æœº\n",
    "            load_best_model_at_end=True,# ç¡®ä¿è®­ç»ƒç»“æŸæ—¶åŠ è½½è¡¨ç°æœ€ä½³çš„æ¨¡å‹\n",
    "            metric_for_best_model=\"eval_loss\",# è¯„åˆ¤ä¿å­˜çš„æ ‡å‡†\n",
    "            save_total_limit=1,# æ¯å½“æ–°æ£€æŸ¥ç‚¹ä¿å­˜æ—¶ä¼šåˆ é™¤æ—§æ£€æŸ¥ç‚¹\n",
    "            report_to=\"wandb\"\n",
    "        )\n",
    "        \n",
    "        # Define a compute_metrics function to calculate and log metrics during training\n",
    "        def compute_metrics(eval_pred):\n",
    "            logits, labels = eval_pred\n",
    "            predictions = np.argmax(logits, axis=-1)\n",
    "            precision, recall, f1, _ = metrics.precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "            acc = metrics.accuracy_score(labels, predictions)\n",
    "            return {\n",
    "                'accuracy': acc,\n",
    "                'f1': f1,\n",
    "                'precision': precision,\n",
    "                'recall': recall\n",
    "            }\n",
    "\n",
    "        # Initialize the Trainer with compute_metrics\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset[\"train\"],\n",
    "            eval_dataset=dataset[\"validation\"] if \"validation\" in dataset else dataset[\"test\"],\n",
    "            tokenizer=self.tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics# metrics for evaluation\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        trainer.train()\n",
    "        training_time = time.time() - start_time\n",
    "\n",
    "        # Log the training time and number of trainable parameters\n",
    "        wandb.log({\n",
    "            \"trainable_params\": model.get_nb_trainable_parameters(),\n",
    "            \"training_time\": training_time\n",
    "        })\n",
    "\n",
    "        wandb.finish()\n",
    "        \n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#  å®šä¹‰é¢å‘å¯¹è±¡çš„ HyperparameterTuner ç±»\n",
    "# ==============================================================================\n",
    "class HyperparameterTuner:\n",
    "    \"\"\"\n",
    "    ä¸€ä¸ªä½¿ç”¨ Optuna è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–çš„å°è£…ç±»ã€‚\n",
    "    \n",
    "    è¿™ä¸ªç±»å°†è¶…å‚æ•°æœç´¢ç©ºé—´å®šä¹‰ã€Optuna studyç®¡ç†å’Œä¼˜åŒ–æ‰§è¡Œè¿‡ç¨‹\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, experiment, direction=\"minimize\", study_name=None, search_space=None):\n",
    "        \"\"\"\n",
    "        æ„é€ å‡½æ•°ã€‚\n",
    "        \n",
    "        Args:\n",
    "            experiment: å®ç°äº† .train_with_peft(...) æ–¹æ³•çš„å®éªŒå¯¹è±¡ã€‚\n",
    "            direction (str): ä¼˜åŒ–çš„æ–¹å‘ï¼Œ\"minimize\" æˆ– \"maximize\"ã€‚\n",
    "            study_name (str, optional): Optuna study çš„åç§°ï¼Œç”¨äºæŒä¹…åŒ–ã€‚\n",
    "            search_space (dict, optional): è¶…å‚æ•°æœç´¢ç©ºé—´é…ç½®ã€‚\n",
    "        \"\"\"\n",
    "        self.experiment = experiment\n",
    "        self.direction = direction\n",
    "        self.study_name = study_name\n",
    "        self.search_space = search_space or {}\n",
    "        \n",
    "        # åœ¨æ„é€ å‡½æ•°ä¸­åˆ›å»º study å¯¹è±¡ï¼Œç®¡ç†æ•´ä¸ªä¼˜åŒ–è¿‡ç¨‹çš„çŠ¶æ€\n",
    "        self.study = optuna.create_study(direction=self.direction, study_name=self.study_name)\n",
    "\n",
    "    def _objective(self, trial):\n",
    "        \"\"\"\n",
    "        é€‰æ‹©å‚æ•°ã€è®­ç»ƒæ¨¡å‹ã€è¿”å›ç»“æœ\n",
    "        \n",
    "        Args:\n",
    "            trial (optuna.trial.Trial): Optuna çš„ trial å¯¹è±¡ï¼Œç”¨äºå»ºè®®å‚æ•°ã€‚\n",
    "            \n",
    "        Returns:\n",
    "            float: éœ€è¦è¢«ä¼˜åŒ–çš„è¯„ä¼°æŒ‡æ ‡ï¼ˆä¾‹å¦‚ï¼ŒéªŒè¯é›†æŸå¤±ï¼‰ã€‚\n",
    "        \"\"\"\n",
    "        # 1. è·å–PEFTæ–¹æ³•æœç´¢ç©ºé—´\n",
    "        peft_space = self.search_space.get('peft')\n",
    "        peft_methods = peft_space.get('methods')\n",
    "        \n",
    "        # é€‰æ‹©PEFTæ–¹æ³•\n",
    "        peft_method = trial.suggest_categorical(\"peft_method\", peft_methods)\n",
    "        \n",
    "        # 2. æ ¹æ®æ–¹æ³•é€‰æ‹©æ¡ä»¶ä¾èµ–çš„å‚æ•°\n",
    "        peft_params = {}\n",
    "        if peft_method == \"lora\":\n",
    "            lora_space = peft_space.get('lora')\n",
    "            peft_params = {\n",
    "                \"r\": trial.suggest_int(\"r\", \n",
    "                                      lora_space.get('r', {}).get('min'), \n",
    "                                      lora_space.get('r', {}).get('max'), \n",
    "                                      step=lora_space.get('r', {}).get('step')),\n",
    "                \"lora_alpha\": trial.suggest_int(\"lora_alpha\", \n",
    "                                               lora_space.get('lora_alpha', {}).get('min'), \n",
    "                                               lora_space.get('lora_alpha', {}).get('max'), \n",
    "                                               step=lora_space.get('lora_alpha', {}).get('step')),\n",
    "                \"lora_dropout\": trial.suggest_float(\"lora_dropout\", \n",
    "                                                   lora_space.get('lora_dropout', {}).get('min'), \n",
    "                                                   lora_space.get('lora_dropout', {}).get('max'))\n",
    "            }\n",
    "        elif peft_method == \"ia3\":\n",
    "            # IA3 æ²¡æœ‰é¢å¤–å‚æ•°\n",
    "            peft_params = {}\n",
    "        else:  # adalora\n",
    "            adalora_space = peft_space.get('adalora', {})\n",
    "            peft_params = {\n",
    "                \"r\": trial.suggest_int(\"r\", \n",
    "                                      adalora_space.get('r', {}).get('min'), \n",
    "                                      adalora_space.get('r', {}).get('max'), \n",
    "                                      step=adalora_space.get('r', {}).get('step')),\n",
    "                \"target_r\": trial.suggest_int(\"target_r\", \n",
    "                                             adalora_space.get('target_r', {}).get('min'), \n",
    "                                             adalora_space.get('target_r', {}).get('max'), \n",
    "                                             step=adalora_space.get('target_r', {}).get('step')),\n",
    "                \"lora_alpha\": trial.suggest_int(\"lora_alpha\", \n",
    "                                               adalora_space.get('lora_alpha', {}).get('min'), \n",
    "                                               adalora_space.get('lora_alpha', {}).get('max'), \n",
    "                                               step=adalora_space.get('lora_alpha', {}).get('step'))\n",
    "            }\n",
    "            \n",
    "        # 3. è·å–è®­ç»ƒå‚æ•°æœç´¢ç©ºé—´\n",
    "        training_space = self.search_space.get('training', {})\n",
    "        \n",
    "        # å®šä¹‰é€šç”¨çš„è®­ç»ƒå‚æ•°\n",
    "        training_params = {\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", \n",
    "                                               training_space.get('learning_rate', {}).get('min'), \n",
    "                                               training_space.get('learning_rate', {}).get('max'), \n",
    "                                               log=training_space.get('learning_rate', {}).get('log')),\n",
    "            \"batch_size\": trial.suggest_categorical(\"batch_size\", \n",
    "                                                  training_space.get('batch_size', {}).get('values')),\n",
    "            \"epochs\": trial.suggest_int(\"epochs\", \n",
    "                                       training_space.get('epochs', {}).get('min'), \n",
    "                                       training_space.get('epochs', {}).get('max')),\n",
    "            \"weight_decay\": trial.suggest_float(\"weight_decay\", \n",
    "                                              training_space.get('weight_decay', {}).get('min'), \n",
    "                                              training_space.get('weight_decay', {}).get('max'), \n",
    "                                              log=training_space.get('weight_decay', {}).get('log'))\n",
    "        }\n",
    "        \n",
    "        # 4. ä½¿ç”¨ self.experiment è°ƒç”¨è®­ç»ƒå’Œè¯„ä¼°\n",
    "        results = self.experiment.train_with_peft(peft_method, peft_params, training_params)\n",
    "        \n",
    "        # 5. è¿”å›ä¼˜åŒ–çš„ç›®æ ‡å€¼\n",
    "        return results[\"eval_loss\"]\n",
    "\n",
    "    def run(self, n_trials):\n",
    "        \"\"\"\n",
    "        å¯åŠ¨è¶…å‚æ•°ä¼˜åŒ–è¿‡ç¨‹ã€‚\n",
    "        \n",
    "        Args:\n",
    "            n_trials (int): è¦è¿è¡Œçš„æ€»è¯•éªŒæ¬¡æ•°ã€‚\n",
    "        \"\"\"\n",
    "        # å°†ç±»æ–¹æ³• _objective ä½œä¸ºä¼˜åŒ–ç›®æ ‡ä¼ å…¥\n",
    "        self.study.optimize(self._objective, n_trials=n_trials)# callback _objective -> Trail injection -> trial_suggest_*:sampler call -> suggested_parameters -> train -> return eval_loss\n",
    "        \n",
    "        print(\"\\nğŸ‰ğŸ‰ğŸ‰ ä¼˜åŒ–å®Œæˆ! ğŸ‰ğŸ‰ğŸ‰\")\n",
    "\n",
    "    def summarize_results(self):\n",
    "        \"\"\"æ‰“å°æœ€æœ‰ç›®æ ‡å€¼åŠå¯¹åº”è¶…å‚æ•°ç»„åˆ\"\"\"\n",
    "        if self.study.best_trial:\n",
    "            print(f\"\\nğŸ“Š æœ€ä½³ç»“æœ:\")\n",
    "            print(f\"  - ç›®æ ‡å€¼ (eval_loss): {self.study.best_value:.4f}\")\n",
    "            print(\"  - æœ€ä½³å‚æ•°ç»„åˆ:\")\n",
    "            for key, value in self.study.best_params.items():\n",
    "                print(f\"    - {key}: {value}\")\n",
    "        else:\n",
    "            print(\"å°šæœªè¿›è¡Œä»»ä½•è¯•éªŒã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_parser = ConfigParser(\"./config.yaml\")\n",
    "exp_config = config_parser.get_experiment_config()\n",
    "peft_config = config_parser.get_peft_config()#.yamlæ–‡ä»¶ä¸­çš„pefté…ç½®\n",
    "training_config = config_parser.get_training_config()#.yamlæ–‡ä»¶ä¸­çš„trainingé…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft config:  {'method': 'lora', 'params': {'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1, 'target_modules': ['q_proj', 'v_proj']}}\n",
      "training config:  {'learning_rate': '5e-4', 'batch_size': 16, 'epochs': 3}\n"
     ]
    }
   ],
   "source": [
    "print(\"peft config: \", peft_config)\n",
    "print(\"training config: \", training_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "My_experiment = PEFTExperiment(\n",
    "        model_name=exp_config['model_name'],\n",
    "        dataset_name=exp_config['dataset_name'],\n",
    "        task_type=exp_config['task_type']\n",
    "        )\n",
    "My_experiment.train_with_peft(\n",
    "    peft_method=peft_config['method'],\n",
    "    peft_params=peft_config['params'],\n",
    "    training_params=training_config\n",
    ")\n",
    "print(f\"è®­ç»ƒå®Œæˆâœ…\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_fine_tune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
