{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "import yaml\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    LoraConfig,\n",
    "    IA3Config,\n",
    "    AdaLoraConfig,\n",
    "    TaskType,\n",
    "    PeftType\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#  ConfigParser类用于解析YAML配置文件\n",
    "# ==============================================================================\n",
    "class ConfigParser:\n",
    "    def __init__(self, config_file):\n",
    "        \"\"\"\n",
    "        初始化配置解析器\n",
    "        \n",
    "        Args:\n",
    "            config_file (str): YAML配置文件的路径\n",
    "        \"\"\"\n",
    "        self.config_file = config_file\n",
    "        self.config = self._load_config()\n",
    "        \n",
    "    def _load_config(self):\n",
    "        \"\"\"加载YAML配置文件\"\"\"\n",
    "        if not os.path.exists(self.config_file):\n",
    "            raise FileNotFoundError(f\"配置文件 {self.config_file} 不存在\")\n",
    "            \n",
    "        with open(self.config_file, 'r') as f:\n",
    "            config = yaml.safe_load(f)# yaml.safe_load()返回一个字典，该字典包含配置文件中的所有键值对\n",
    "        return config\n",
    "    \n",
    "    def get_experiment_config(self):\n",
    "        \"\"\"获取实验基本配置\"\"\"\n",
    "        exp_config = self.config.get('experiment')\n",
    "        return {\n",
    "            'model_name': exp_config.get('model_name'),\n",
    "            'dataset_name': exp_config.get('dataset_name'),\n",
    "            'task_type': exp_config.get('task_type')\n",
    "        }\n",
    "    \n",
    "    def get_peft_config(self):\n",
    "        \"\"\"获取PEFT方法和参数配置\"\"\"\n",
    "        peft_config = self.config.get('peft')\n",
    "        return {\n",
    "            'method': peft_config.get('method'),\n",
    "            'params': peft_config.get('params')\n",
    "        }\n",
    "    \n",
    "    def get_training_config(self):\n",
    "        \"\"\"获取训练参数配置\"\"\"\n",
    "        return self.config.get('training')\n",
    "    \n",
    "    def get_tuner_config(self):\n",
    "        \"\"\"获取超参数优化器配置\"\"\"\n",
    "        tuner_config = self.config.get('tuner')\n",
    "        return {\n",
    "            'direction': tuner_config.get('direction'),\n",
    "            'n_trials': tuner_config.get('n_trials'),\n",
    "            'study_name': tuner_config.get('study_name')\n",
    "        }\n",
    "    \n",
    "    def get_peft_search_space(self):\n",
    "        \"\"\"获取PEFT方法的搜索空间配置\"\"\"\n",
    "        return self.config.get('search_space').get('peft')\n",
    "    \n",
    "    def get_training_search_space(self):\n",
    "        \"\"\"获取训练参数的搜索空间配置\"\"\"\n",
    "        return self.config.get('search_space').get('training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#  PEFTExperiment类用于执行PEFT训练和评估\n",
    "# ==============================================================================\n",
    "class PEFTExperiment:\n",
    "    def __init__(self, model_name, dataset_name, task_type):\n",
    "        self.model_name = model_name\n",
    "        self.dataset_name = dataset_name\n",
    "        self.task_type = task_type\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            print(\"pad_token is None, set to eos_token\")\n",
    "    \n",
    "    def get_peft_config(self, method, **kwargs):\n",
    "        \"\"\"获取不同PEFT方法的配置\"\"\"\n",
    "        configs = {\n",
    "            \"lora\": LoraConfig(\n",
    "                task_type=self.task_type,\n",
    "                inference_mode=False,\n",
    "                r=kwargs.get(\"r\"),\n",
    "                lora_alpha=kwargs.get(\"lora_alpha\"),# 决定delta_W的影响\n",
    "                lora_dropout=kwargs.get(\"lora_dropout\"),\n",
    "                target_modules=kwargs.get(\"target_modules\")\n",
    "            ),\n",
    "            \"ia3\": IA3Config(\n",
    "                task_type=self.task_type,\n",
    "                inference_mode=False,\n",
    "                target_modules=kwargs.get(\"target_modules\"),\n",
    "                feedforward_modules=kwargs.get(\"feedforward_modules\")\n",
    "            ),\n",
    "            \"adalora\": AdaLoraConfig(\n",
    "                task_type=self.task_type,\n",
    "                inference_mode=False,\n",
    "                r=kwargs.get(\"r\"),\n",
    "                lora_alpha=kwargs.get(\"lora_alpha\"),\n",
    "                target_r=kwargs.get(\"target_r\"),\n",
    "                init_r=kwargs.get(\"init_r\"),\n",
    "                tinit=kwargs.get(\"tinit\"),\n",
    "                tfinal=kwargs.get(\"tfinal\"),\n",
    "                deltaT=kwargs.get(\"deltaT\"),\n",
    "                lora_dropout=kwargs.get(\"lora_dropout\"),\n",
    "                target_modules=kwargs.get(\"target_modules\")\n",
    "            )\n",
    "        }\n",
    "        return configs[method.lower()]\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        \"\"\"准备数据集\"\"\"\n",
    "        dataset = load_dataset(self.dataset_name)\n",
    "        \n",
    "        def tokenize_function(examples):\n",
    "            return self.tokenizer(\n",
    "                examples[\"text\"] if \"text\" in examples else examples[\"sentence\"],\n",
    "                truncation=True,\n",
    "                padding=False,\n",
    "                max_length=512\n",
    "            )\n",
    "        \n",
    "        tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "        return tokenized_dataset\n",
    "    \n",
    "    def train_with_peft(self, peft_method, peft_params, training_params):\n",
    "        \"\"\"使用PEFT方法训练模型\"\"\"\n",
    "\n",
    "        # 初始化wandb\n",
    "        run = wandb.init(\n",
    "            project=\"DNA_LLM_finetune\",\n",
    "            name=f\"{peft_method}_{self.model_name}_{self.dataset_name}\",\n",
    "            config={\n",
    "                \"model\": self.model_name,\n",
    "                \"peft_method\": peft_method,\n",
    "                \"peft_params\": peft_params,#参数高效微调超参数\n",
    "                **training_params#训练超参数\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # 加载模型\n",
    "        model = AutoModel.from_pretrained(\n",
    "            self.model_name,\n",
    "            num_labels=2  # 根据任务调整\n",
    "        )\n",
    "        \n",
    "        # 应用PEFT配置\n",
    "        peft_config = self.get_peft_config(peft_method, **peft_params)\n",
    "        model = get_peft_model(model, peft_config)#应用配置后的PEFT模型\n",
    "        \n",
    "        # 准备数据\n",
    "        dataset = self.prepare_data()\n",
    "        data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)#动态padding一个batch中的所有样本\n",
    "        \n",
    "        # 训练参数\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"./results/{peft_method}_{run.id}\",\n",
    "            learning_rate=training_params.get(\"learning_rate\"),\n",
    "            per_device_train_batch_size=training_params.get(\"batch_size\"),\n",
    "            num_train_epochs=training_params.get(\"epochs\"),\n",
    "            weight_decay=training_params.get(\"weight_decay\"),\n",
    "            logging_steps=10,\n",
    "            evaluation_strategy=\"epoch\",# 定义evaluation时机\n",
    "            save_strategy=\"epoch\",# 定义模型保存时机\n",
    "            load_best_model_at_end=True,# 确保训练结束时加载表现最佳的模型\n",
    "            metric_for_best_model=\"eval_loss\",# 评判保存的标准\n",
    "            save_total_limit=1,# 每当新检查点保存时会删除旧检查点\n",
    "            report_to=\"wandb\"\n",
    "        )\n",
    "        \n",
    "        # Define a compute_metrics function to calculate and log metrics during training\n",
    "        def compute_metrics(eval_pred):\n",
    "            logits, labels = eval_pred\n",
    "            predictions = np.argmax(logits, axis=-1)\n",
    "            precision, recall, f1, _ = metrics.precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "            acc = metrics.accuracy_score(labels, predictions)\n",
    "            return {\n",
    "                'accuracy': acc,\n",
    "                'f1': f1,\n",
    "                'precision': precision,\n",
    "                'recall': recall\n",
    "            }\n",
    "\n",
    "        # Initialize the Trainer with compute_metrics\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset[\"train\"],\n",
    "            eval_dataset=dataset[\"validation\"] if \"validation\" in dataset else dataset[\"test\"],\n",
    "            tokenizer=self.tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics# metrics for evaluation\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        trainer.train()\n",
    "        training_time = time.time() - start_time\n",
    "\n",
    "        # Log the training time and number of trainable parameters\n",
    "        wandb.log({\n",
    "            \"trainable_params\": model.get_nb_trainable_parameters(),\n",
    "            \"training_time\": training_time\n",
    "        })\n",
    "\n",
    "        wandb.finish()\n",
    "        \n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#  定义面向对象的 HyperparameterTuner 类\n",
    "# ==============================================================================\n",
    "class HyperparameterTuner:\n",
    "    \"\"\"\n",
    "    一个使用 Optuna 进行超参数优化的封装类。\n",
    "    \n",
    "    这个类将超参数搜索空间定义、Optuna study管理和优化执行过程\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, experiment, direction=\"minimize\", study_name=None, search_space=None):\n",
    "        \"\"\"\n",
    "        构造函数。\n",
    "        \n",
    "        Args:\n",
    "            experiment: 实现了 .train_with_peft(...) 方法的实验对象。\n",
    "            direction (str): 优化的方向，\"minimize\" 或 \"maximize\"。\n",
    "            study_name (str, optional): Optuna study 的名称，用于持久化。\n",
    "            search_space (dict, optional): 超参数搜索空间配置。\n",
    "        \"\"\"\n",
    "        self.experiment = experiment\n",
    "        self.direction = direction\n",
    "        self.study_name = study_name\n",
    "        self.search_space = search_space or {}\n",
    "        \n",
    "        # 在构造函数中创建 study 对象，管理整个优化过程的状态\n",
    "        self.study = optuna.create_study(direction=self.direction, study_name=self.study_name)\n",
    "\n",
    "    def _objective(self, trial):\n",
    "        \"\"\"\n",
    "        选择参数、训练模型、返回结果\n",
    "        \n",
    "        Args:\n",
    "            trial (optuna.trial.Trial): Optuna 的 trial 对象，用于建议参数。\n",
    "            \n",
    "        Returns:\n",
    "            float: 需要被优化的评估指标（例如，验证集损失）。\n",
    "        \"\"\"\n",
    "        # 1. 获取PEFT方法搜索空间\n",
    "        peft_space = self.search_space.get('peft')\n",
    "        peft_methods = peft_space.get('methods')\n",
    "        \n",
    "        # 选择PEFT方法\n",
    "        peft_method = trial.suggest_categorical(\"peft_method\", peft_methods)\n",
    "        \n",
    "        # 2. 根据方法选择条件依赖的参数\n",
    "        peft_params = {}\n",
    "        if peft_method == \"lora\":\n",
    "            lora_space = peft_space.get('lora')\n",
    "            peft_params = {\n",
    "                \"r\": trial.suggest_int(\"r\", \n",
    "                                      lora_space.get('r', {}).get('min'), \n",
    "                                      lora_space.get('r', {}).get('max'), \n",
    "                                      step=lora_space.get('r', {}).get('step')),\n",
    "                \"lora_alpha\": trial.suggest_int(\"lora_alpha\", \n",
    "                                               lora_space.get('lora_alpha', {}).get('min'), \n",
    "                                               lora_space.get('lora_alpha', {}).get('max'), \n",
    "                                               step=lora_space.get('lora_alpha', {}).get('step')),\n",
    "                \"lora_dropout\": trial.suggest_float(\"lora_dropout\", \n",
    "                                                   lora_space.get('lora_dropout', {}).get('min'), \n",
    "                                                   lora_space.get('lora_dropout', {}).get('max'))\n",
    "            }\n",
    "        elif peft_method == \"ia3\":\n",
    "            # IA3 没有额外参数\n",
    "            peft_params = {}\n",
    "        else:  # adalora\n",
    "            adalora_space = peft_space.get('adalora', {})\n",
    "            peft_params = {\n",
    "                \"r\": trial.suggest_int(\"r\", \n",
    "                                      adalora_space.get('r', {}).get('min'), \n",
    "                                      adalora_space.get('r', {}).get('max'), \n",
    "                                      step=adalora_space.get('r', {}).get('step')),\n",
    "                \"target_r\": trial.suggest_int(\"target_r\", \n",
    "                                             adalora_space.get('target_r', {}).get('min'), \n",
    "                                             adalora_space.get('target_r', {}).get('max'), \n",
    "                                             step=adalora_space.get('target_r', {}).get('step')),\n",
    "                \"lora_alpha\": trial.suggest_int(\"lora_alpha\", \n",
    "                                               adalora_space.get('lora_alpha', {}).get('min'), \n",
    "                                               adalora_space.get('lora_alpha', {}).get('max'), \n",
    "                                               step=adalora_space.get('lora_alpha', {}).get('step'))\n",
    "            }\n",
    "            \n",
    "        # 3. 获取训练参数搜索空间\n",
    "        training_space = self.search_space.get('training', {})\n",
    "        \n",
    "        # 定义通用的训练参数\n",
    "        training_params = {\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", \n",
    "                                               training_space.get('learning_rate', {}).get('min'), \n",
    "                                               training_space.get('learning_rate', {}).get('max'), \n",
    "                                               log=training_space.get('learning_rate', {}).get('log')),\n",
    "            \"batch_size\": trial.suggest_categorical(\"batch_size\", \n",
    "                                                  training_space.get('batch_size', {}).get('values')),\n",
    "            \"epochs\": trial.suggest_int(\"epochs\", \n",
    "                                       training_space.get('epochs', {}).get('min'), \n",
    "                                       training_space.get('epochs', {}).get('max')),\n",
    "            \"weight_decay\": trial.suggest_float(\"weight_decay\", \n",
    "                                              training_space.get('weight_decay', {}).get('min'), \n",
    "                                              training_space.get('weight_decay', {}).get('max'), \n",
    "                                              log=training_space.get('weight_decay', {}).get('log'))\n",
    "        }\n",
    "        \n",
    "        # 4. 使用 self.experiment 调用训练和评估\n",
    "        results = self.experiment.train_with_peft(peft_method, peft_params, training_params)\n",
    "        \n",
    "        # 5. 返回优化的目标值\n",
    "        return results[\"eval_loss\"]\n",
    "\n",
    "    def run(self, n_trials):\n",
    "        \"\"\"\n",
    "        启动超参数优化过程。\n",
    "        \n",
    "        Args:\n",
    "            n_trials (int): 要运行的总试验次数。\n",
    "        \"\"\"\n",
    "        # 将类方法 _objective 作为优化目标传入\n",
    "        self.study.optimize(self._objective, n_trials=n_trials)# callback _objective -> Trail injection -> trial_suggest_*:sampler call -> suggested_parameters -> train -> return eval_loss\n",
    "        \n",
    "        print(\"\\n🎉🎉🎉 优化完成! 🎉🎉🎉\")\n",
    "\n",
    "    def summarize_results(self):\n",
    "        \"\"\"打印最有目标值及对应超参数组合\"\"\"\n",
    "        if self.study.best_trial:\n",
    "            print(f\"\\n📊 最佳结果:\")\n",
    "            print(f\"  - 目标值 (eval_loss): {self.study.best_value:.4f}\")\n",
    "            print(\"  - 最佳参数组合:\")\n",
    "            for key, value in self.study.best_params.items():\n",
    "                print(f\"    - {key}: {value}\")\n",
    "        else:\n",
    "            print(\"尚未进行任何试验。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_parser = ConfigParser(\"./config.yaml\")\n",
    "exp_config = config_parser.get_experiment_config()\n",
    "peft_config = config_parser.get_peft_config()#.yaml文件中的peft配置\n",
    "training_config = config_parser.get_training_config()#.yaml文件中的training配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft config:  {'method': 'lora', 'params': {'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1, 'target_modules': ['q_proj', 'v_proj']}}\n",
      "training config:  {'learning_rate': '5e-4', 'batch_size': 16, 'epochs': 3}\n"
     ]
    }
   ],
   "source": [
    "print(\"peft config: \", peft_config)\n",
    "print(\"training config: \", training_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "My_experiment = PEFTExperiment(\n",
    "        model_name=exp_config['model_name'],\n",
    "        dataset_name=exp_config['dataset_name'],\n",
    "        task_type=exp_config['task_type']\n",
    "        )\n",
    "My_experiment.train_with_peft(\n",
    "    peft_method=peft_config['method'],\n",
    "    peft_params=peft_config['params'],\n",
    "    training_params=training_config\n",
    ")\n",
    "print(f\"训练完成✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_fine_tune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
